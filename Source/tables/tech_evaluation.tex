\begin{table*}[ht]
\centering
\begin{minipage}[t]{0.47\textwidth}
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method}       & \textbf{IoU}   & \textbf{Precision} & \textbf{Recall} & \textbf{F1}    \\ \midrule
\textbf{\texttt{Ours}}  & \textbf{0.543} & 0.607     & \textbf{0.902}  & \textbf{0.726} \\
\textbf{\texttt{Rating}} & 0.414          & \textbf{0.615}              & 0.843           & 0.711          \\ \bottomrule
\end{tabular}%
\caption{Performance of the tested methods in fragment extraction as measured by the Intersection-over-Union (IoU) of predicted and ground-truth fragments, and precision, recall, and F1-score of the predicted fragments.}
\Description{This table compares the performance of two methods, Ours and Rating, in fragment extraction. It reports four metrics: Intersection-over-Union (IoU), Precision, Recall, and F1-score.
Our method achieves higher IoU (0.543 vs. 0.414) and Recall (0.902 vs. 0.843), while Rating method shows slightly higher Precision (0.615 vs. 0.607). Overall, our approach obtains the best F1-score of 0.726 compared to 0.711 for the baseline.}
\label{tab:tech_fragment}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.47\textwidth}
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method}       & \textbf{Overall} & \textbf{Chat}  & \textbf{Chat-Hard} & \textbf{Safety} \\ \midrule
\textbf{\texttt{Ours}}  & \textbf{0.801}            & \textbf{0.842} & \textbf{0.559}              & \textbf{0.849}           \\
\textbf{\texttt{Rating}} & 0.755   & 0.741          & 0.529     & 0.831  \\ \bottomrule
\end{tabular}%
\caption{Performance of the tested methods in terms of their accuracy at identifying the higher quality outputs from a pair of LLM-generated outputs. The table shows the accuracy for the whole dataset and for each subset.}
\Description{This table evaluates the accuracy of two methods, Ours and Rating, at selecting higher-quality outputs from LLM-generated pairs. The evaluation is reported across four settings: Overall, Chat, Chat-Hard, and Safety. Our method outperforms the baseline in all categories, achieving the highest accuracy overall (0.801 vs. 0.755), as well as in Chat (0.842 vs. 0.741), Chat-Hard (0.559 vs. 0.529), and Safety (0.849 vs. 0.831).}
\label{tab:tech_overall}
\end{minipage}
\end{table*}