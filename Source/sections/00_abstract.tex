\begin{abstract}
    Practitioners increasingly rely on Large Language Models (LLMs) to evaluate generative AI outputs through "LLM-as-a-Judge" approaches. However, these methods produce holistic scores that obscure which specific elements influenced the assessments. We propose \approach{}, a method that dissects each output into key fragments and interprets the rhetoric functions that each fragment serves relative to evaluation criteriaâ€”surfacing the elements of interest and revealing how they fulfill or hinder user goals. We instantiate this approach in \sysname{}, an interactive system that visualizes fragment-level functions across many outputs to support inspection, rating, and comparison of evaluations. A user study (N=10) found that, while practitioners struggled to validate holistic scores, our approach helped them identify 48\% more evaluation misalignments. This helped them calibrate trust in LLM evaluations and rely on them to find more actionable issues in model outputs. Our work shifts LLM evaluation from quantitative scores toward qualitative, fine-grained analysis of model behavior.
\end{abstract}